{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate automated session reports for all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import mne_bids\n",
    "import glob\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "import q1k_pyll_tools as qpt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select task parameters and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select kind of data experimental or control group\n",
    "\n",
    "dataset_group = \"experimental\"\n",
    "\n",
    "if dataset_group == \"control\":\n",
    "    # Control group data\n",
    "    project_path = \"/home/james/q1k/pilot/q1k-external-pilot/\"\n",
    "    task_id_in = \"ap\"\n",
    "    task_id_in_et = \"ap\"\n",
    "    task_id_out = \"ap\"\n",
    "    #subject_id = '002'\n",
    "    session_id = '01'\n",
    "    run_id = '1'\n",
    "\n",
    "elif dataset_group == \"experimental\":\n",
    "# Experimental group data\n",
    "\n",
    "    project_path = \"/project/def-emayada/q1k/experimental/HSJ/\"\n",
    "    init_path = \"code/q1k_eeget_init/\"\n",
    "    task_id_in = \"PLR\"\n",
    "    task_id_in_et = \"PLR\"\n",
    "    task_id_out = \"PLR\"\n",
    "    #subject_id = 'Q1K_HSJ_100123_F1'\n",
    "    run_id = '1'\n",
    "    session_id = '01'\n",
    "    root_data_path = \"\" \n",
    "    derivative_path = \"derivatives/pylossless/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Function to extract details from filename\n",
    "#def extract_job_info(filename):\n",
    "#    # Define the pattern to capture the required sections\n",
    "#    pattern = r\"sub-(.*?)_ses-(.*?)_task-(.*?)_run-(.*?)_eeg\\.edf\"\n",
    "#    match = re.match(pattern, filename)\n",
    "\n",
    "#    if match:\n",
    "#        # Extract the groups from the match\n",
    "#        subject_id = match.group(1)\n",
    "#        session_id = match.group(2)\n",
    "#        task_id = match.group(3)\n",
    "#        run_id = match.group(4)\n",
    "#        return subject_id, session_id, task_id, run_id\n",
    "#    else:\n",
    "#        raise ValueError(\"Filename pattern did not match.\")\n",
    "\n",
    "## Function to submit job to Slurm scheduler\n",
    "#def submit_slurm_job(project_path, filename):\n",
    "#    # Extract job info from filename\n",
    "#    subject_id, session_id, task_id, run_id = extract_job_info(filename)\n",
    "    \n",
    "#    # Construct the sbatch command\n",
    "#    sbatch_command = [\n",
    "#        'sbatch',\n",
    "#        'q1k_pyll_jobsub.sh',  # Your Slurm script name\n",
    "#        project_path,         # Project path\n",
    "#        subject_id,           # Extracted subject_id\n",
    "#        session_id,           # Extracted session_id\n",
    "#        task_id,              # Extracted task_id\n",
    "#        run_id,               # Extracted run_id\n",
    "#        project_path          # Project path\n",
    "#    ]\n",
    "\n",
    "#    # Print the command to check it before submission\n",
    "#    print(\"Running command:\", ' '.join(sbatch_command))\n",
    "\n",
    "#    # Submit the job using subprocess\n",
    "#    try:\n",
    "#        result = subprocess.run(sbatch_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#        print(\"Job submitted successfully!\")\n",
    "#        print(result.stdout.decode())\n",
    "#    except subprocess.CalledProcessError as e:\n",
    "#        print(\"Error submitting job:\", e.stderr.decode())\n",
    "\n",
    "\n",
    "pattern = project_path + derivative_path + '**/eeg/*' + task_id_out + '*.edf'\n",
    "processed_sessions = glob.glob(pattern, recursive=True)\n",
    "        \n",
    "pattern = project_path + '**/eeg/*' + task_id_in + '*.edf'\n",
    "file_paths = glob.glob(pattern, recursive=True)\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)  # Extract the filename from the full path\n",
    "    print(f\"File Path: {file_path}, File Name: {file_name}\")\n",
    "    \n",
    "    # Skip sessions that have already been processed\n",
    "    if file_path in processed_sessions:\n",
    "        print(file_name + ' has already been processed')\n",
    "        continue    \n",
    "\n",
    "\n",
    "    sbatch_command, result = qpt.submit_slurm_job(project_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "q1k_env",
   "language": "python",
   "name": "q1k_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
